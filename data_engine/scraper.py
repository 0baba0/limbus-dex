import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# ì €ì¥ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
WEB_SITE_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "web_site"))
IMAGE_DIR = os.path.join(WEB_SITE_DIR, "public", "images", "characters")

os.makedirs(IMAGE_DIR, exist_ok=True)

def download_fallback_images():
    print("ğŸ›¡ï¸ ë„¤ì´ë²„ ë°©ì–´ë§‰ ìš°íšŒ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    TARGET_URL = "https://m.blog.naver.com/baeeunhye13/223055148829"
    
    # ì›¹í˜ì´ì§€ ì ‘ì†ìš© í—¤ë”
    page_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì „ìš© í—¤ë” (ë„¤ì´ë²„ ì´ë¯¸ì§€ ì„œë²„ê°€ ì¢‹ì•„í•˜ëŠ” PC ë²„ì „ Refererë¡œ ìœ„ì¥)
    img_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": "https://blog.naver.com/" 
    }

    try:
        response = requests.get(TARGET_URL, headers=page_headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        images = soup.select('img.se-image-resource')
        print(f"âœ… ì´ {len(images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!\n")

        for i, img in enumerate(images):
            # 1. URL ì¶”ì¶œ (ì§€ì—° ë¡œë”© ëŒ€ì‘)
            raw_url = img.get('data-lazy-src') or img.get('src')
            if not raw_url:
                continue
            
            clean_url = raw_url.split('?')[0] # ì›ë³¸ ì‹œë„ìš© URL
            
            # í™•ì¥ì ì¶”ì¶œ ë° íŒŒì¼ëª… ì§€ì •
            parsed_url = urlparse(clean_url)
            ext = os.path.splitext(parsed_url.path)[1]
            if not ext: ext = ".png"
            
            filename = f"limbus_image_{i+1:03d}{ext}"
            filepath = os.path.join(IMAGE_DIR, filename)

            # 2. [í”Œëœ A] íŒŒë¼ë¯¸í„° ë—€ ì›ë³¸ URLë¡œ ë¨¼ì € ë‹¤ìš´ë¡œë“œ ì‹œë„
            img_response = requests.get(clean_url, headers=img_headers)
            
            # 3. [í”Œëœ B] ë§Œì•½ ë§‰í˜”ë‹¤ë©´(403, 404), ì›ë˜ URL(raw_url)ë¡œ ì¬ì‹œë„
            if img_response.status_code != 200:
                img_response = requests.get(raw_url, headers=img_headers)

            # 4. ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
            if img_response.status_code == 200:
                with open(filepath, "wb") as f:
                    f.write(img_response.content)
                print(f"  -> ğŸ’¾ [{filename}] ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
            else:
                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ ì½”ë“œë¥¼ ì¶œë ¥í•´ì„œ ë””ë²„ê¹…í•  ìˆ˜ ìˆê²Œ í•¨
                print(f"  -> âŒ [{filename}] ìµœì¢… ì‹¤íŒ¨ (ì—ëŸ¬ ì½”ë“œ: {img_response.status_code})")

        print("\nğŸ‰ ë‹¤ìš´ë¡œë“œ ì‘ì—…ì´ ëë‚¬ì–´! í´ë”ë¥¼ í™•ì¸í•´ ë´.")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    download_fallback_images()